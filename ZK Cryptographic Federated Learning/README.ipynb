{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c233c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1071368364.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThis project implements a privacy-preserving, cryptographically enhanced Federated Learning (FL) framework using JAX, Flax, and Optax. It supports multiple neural network architectures (LeNet, CNN, ResNet) and simulates a federated environment with configurable data distributions, client activity, and quantization for communication efficiency. The project is designed for research and experimentation in distributed, privacy-aware machine learning.\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ZK Cryptographic Federated Learning\n",
    "\n",
    "## Overview\n",
    "This project implements a privacy-preserving, cryptographically enhanced Federated Learning (FL) framework using JAX, Flax, and Optax. It supports multiple neural network architectures (LeNet, CNN, ResNet) and simulates a federated environment with configurable data distributions, client activity, and quantization for communication efficiency. The project is designed for research and experimentation in distributed, privacy-aware machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical and Technical Background\n",
    "\n",
    "### Federated Learning (FL)\n",
    "Federated Learning is a distributed machine learning paradigm where multiple clients (workers) collaboratively train a model under the orchestration of a central server, while keeping their data local. The server aggregates model updates from clients, improving privacy and reducing data transfer.\n",
    "\n",
    "#### Federated Optimization Objective\n",
    "The global objective in FL is to minimize the sum of local objectives:\n",
    "$$\n",
    "\\min_{\\theta} F(\\theta) = \\sum_{k=1}^K p_k F_k(\\theta)\n",
    "$$\n",
    "where:\n",
    "- $ K $: Number of clients\n",
    "- $ p_k = \\frac{n_k}{n} $: Proportion of data on client $ k $\n",
    "- $ F_k(\\theta) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\ell(\\theta; x_i, y_i) $: Local loss on client $ k $\n",
    "- $ \\ell $: Loss function (e.g., cross-entropy)\n",
    "\n",
    "#### Federated Averaging (FedAvg)\n",
    "At each round (server epoch):\n",
    "1. The server sends the current model parameters $ \\theta $ to all clients.\n",
    "2. Each client updates $ \\theta $ using its local data for several epochs, producing $ \\theta_k $.\n",
    "3. The server aggregates the updates, typically by weighted averaging:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\sum_{k=1}^K p_k \\theta_k\n",
    "   $$\n",
    "\n",
    "#### Quantization for Communication Efficiency\n",
    "To reduce communication cost, model updates are quantized before being sent to the server. This project implements **stochastic quantization**:\n",
    "- For a value $ x \\in [0, 1] $ and quantization level $ q $,\n",
    "- Let $ \\lfloor qx \\rfloor $ be the lower quantization bin, and $ \\alpha = qx - \\lfloor qx \\rfloor $ the fractional part.\n",
    "- With probability $ \\alpha $, round up; otherwise, round down:\n",
    "$$\n",
    "Q(x) = \\begin{cases}\n",
    "\\frac{\\lfloor qx \\rfloor + 1}{q} & \\text{with probability } \\alpha \\\\\n",
    "\\frac{\\lfloor qx \\rfloor}{q} & \\text{with probability } 1-\\alpha\n",
    "\\end{cases}\n",
    "$$\n",
    "This preserves the expectation: $ \\mathbb{E}[Q(x)] = x $.\n",
    "\n",
    "#### Dirichlet Data Partitioning (Non-IID Data)\n",
    "To simulate realistic, non-IID data distributions across clients, the project supports Dirichlet partitioning:\n",
    "- For $ K $ clients and $ C $ classes, draw proportions $ \\mathbf{p}_k \\sim \\text{Dirichlet}(\\alpha) $ for each class.\n",
    "- Assign data to clients according to these proportions, controlling heterogeneity with $ \\alpha $:\n",
    "  - Small $ \\alpha $: Highly non-IID (clients see few classes)\n",
    "  - Large $ \\alpha $: Nearly IID\n",
    "\n",
    "#### Client Activity and Dropout\n",
    "Client activity is modeled as a Bernoulli process:\n",
    "$$\n",
    "A_{k,t} \\sim \\text{Bernoulli}(1 - p_{\\text{inact}})\n",
    "$$\n",
    "where $ A_{k,t} = 1 $ if client $ k $ is active at round $ t $.\n",
    "\n",
    "---\n",
    "\n",
    "## Cryptographic Details: Secure Aggregation and Quantization\n",
    "\n",
    "### Secure Aggregation (Conceptual)\n",
    "While this codebase does not implement a full cryptographic protocol, it is designed to be extensible for secure aggregation, which is crucial for privacy in FL. The main idea is:\n",
    "- Each client masks its update with random values (additive secret sharing) so that the server cannot see individual updates, only their sum.\n",
    "- The server receives masked updates and, after all masks cancel out, recovers the aggregate update.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- Each client $ k $ computes update $ \\Delta_k $ and mask $ r_k $.\n",
    "- Sends $ \\Delta_k + r_k $ to the server.\n",
    "- Masks are constructed so that $ \\sum_k r_k = 0 $, thus:\n",
    "$$\n",
    "\\sum_k (\\Delta_k + r_k) = \\sum_k \\Delta_k\n",
    "$$\n",
    "\n",
    "### Quantization with Modular Arithmetic (for Secure Aggregation)\n",
    "- Quantized updates can be mapped to a finite field (e.g., modulo a prime $ p $), enabling cryptographic protocols:\n",
    "$$\n",
    "Q(x) = \\text{Quantize}(x) \\mod p\n",
    "$$\n",
    "- This is useful for protocols like [Bonawitz et al., 2017](https://arxiv.org/abs/1611.04482), where aggregation is performed in a finite field.\n",
    "\n",
    "### Zero-Knowledge Proofs (ZK) (Research Direction)\n",
    "- The project is structured to allow future integration of ZK proofs, where clients can prove properties about their updates (e.g., correct computation, bounded norm) without revealing the updates themselves.\n",
    "- ZK proofs are not yet implemented (in progress), but the quantization and modular arithmetic steps are compatible with such extensions.\n",
    "\n",
    "---\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "ZK Cryptographic Federated Learning/\n",
    "├── Main.py                # Main entry point, orchestrates FL workflow\n",
    "├── Training.py            # Client/server training logic, quantization\n",
    "├── Utils.py               # Data loading, client management, helper functions\n",
    "├── Commons.py             # Shared types, config, and constants\n",
    "├── Models/                # Model architectures (LeNet, CNN, ResNet)\n",
    "│   ├── lenet.py\n",
    "│   ├── cnn.py\n",
    "│   ├── resnet.py\n",
    "│   └── __init__.py\n",
    "├── Params/                # Experiment configuration files\n",
    "│   └── Parameters_1.py\n",
    "├── Logs/                  # Output logs and data distribution plots\n",
    "│   └── _Data_Distribution.png\n",
    "├── wandb/                 # Weights & Biases experiment tracking\n",
    "├── Playground.ipynb       # Interactive notebook for experimentation\n",
    "└── ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Component Details\n",
    "\n",
    "### 1. Main Workflow (`Main.py`)\n",
    "- Loads experiment configurations from `Params/`.\n",
    "- Initializes random seeds, datasets, and clients.\n",
    "- Distributes data among clients according to configuration (IID, non-IID, Dirichlet, etc.).\n",
    "- For each server epoch:\n",
    "  - Selects active clients (can simulate client dropout/inactivity).\n",
    "  - Each active client trains locally for a configurable number of epochs.\n",
    "  - Aggregates client updates (FedAvg or other schemes).\n",
    "  - Optionally applies quantization to updates.\n",
    "  - Evaluates and logs validation/test metrics.\n",
    "- Logs results to Weights & Biases (wandb) and saves CSV logs.\n",
    "\n",
    "### 2. Models (`Models/`)\n",
    "- **LeNet5 (`lenet.py`)**: Classic CNN for digit recognition. Layers: Conv → ReLU → Pool → Conv → ReLU → Pool → Dense → ReLU → Dense → ReLU → Dense.\n",
    "- **CNNs (`cnn.py`)**: Several deeper CNNs for more complex datasets (e.g., CIFAR-10). Includes variants with 4, 6, or 10 dense layers.\n",
    "- **ResNet (`resnet.py`)**: Modern deep residual networks (ResNet18, 34, 50, etc.) with skip connections, batch normalization, and flexible depth.\n",
    "- All models are implemented using Flax's `nn.Module` and are selected via the `get_model` function in `Models/__init__.py`.\n",
    "\n",
    "### 3. Training Logic (`Training.py`)\n",
    "- **train_client**: Each client receives the global model, trains locally, and returns updated parameters (optionally quantized).\n",
    "- **test_client**: Evaluates a model on test/validation data.\n",
    "- **Quantization**: Implements stochastic quantization for communication-efficient FL. Quantizes gradients/updates to a fixed number of levels, optionally modulo a prime (for secure aggregation research).\n",
    "- **JAX JIT**: Training and update steps are JIT-compiled for speed.\n",
    "\n",
    "### 4. Utilities (`Utils.py`)\n",
    "- **Client Class**: Represents a federated client, holding local data, labels, and state.\n",
    "- **get_datasets**: Loads and normalizes datasets (MNIST, CIFAR-10, Fashion-MNIST, EMNIST, etc.).\n",
    "- **Data Distribution**: Functions to split data by class, sample validation sets, and allocate data to clients (supports various non-IID schemes).\n",
    "- **Client Activity**: Simulates client dropout/inactivity per epoch.\n",
    "- **Logging and Plotting**: Logs metrics, plots data distributions, and manages experiment metadata.\n",
    "\n",
    "### 5. Configuration (`Params/Parameters_1.py`)\n",
    "- Uses `ml_collections.ConfigDict` for hierarchical, flexible configuration.\n",
    "- Controls all aspects: model, optimizer, data, server/client settings, quantization, poisoning, etc.\n",
    "- Example settings:\n",
    "  - `config.train.model`: Model architecture (e.g., LeNet5)\n",
    "  - `config.data.name`: Dataset name\n",
    "  - `config.server.num_epochs`: Number of global rounds\n",
    "  - `config.worker.num`: Number of clients\n",
    "  - `config.quantization.levels`: Quantization granularity\n",
    "\n",
    "### 6. Experiment Tracking (`wandb/`)\n",
    "- Integrates with [Weights & Biases](https://wandb.ai/) for experiment tracking, logging, and visualization.\n",
    "- Each run logs metrics, configuration, and outputs for reproducibility.\n",
    "\n",
    "### 7. Playground (`Playground.ipynb`)\n",
    "- Jupyter notebook for interactive experimentation, visualization, and debugging.\n",
    "- Walks through the main workflow, data loading, client setup, and model training.\n",
    "\n",
    "---\n",
    "\n",
    "## How Components Interact\n",
    "\n",
    "- **Main.py** orchestrates the entire workflow, calling utility functions for data and client setup, and invoking training logic for each round.\n",
    "- **Models** are selected and instantiated based on configuration, and passed to training functions.\n",
    "- **Training.py** handles all local and global training logic, including quantization and evaluation.\n",
    "- **Utils.py** provides all data handling, client management, and logging utilities.\n",
    "- **Parameters_1.py** (and other config files) allow easy switching between experimental setups.\n",
    "- **wandb** logs all relevant metrics and artifacts for later analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Running the Project\n",
    "\n",
    "### 1. Install Dependencies\n",
    "Install all required packages (see `wandb/run-*/files/requirements.txt` for full list):\n",
    "```bash\n",
    "pip install -r wandb/run-20250216_221830-67fnv1p9/files/requirements.txt\n",
    "```\n",
    "\n",
    "### 2. Run Main Experiment\n",
    "```bash\n",
    "python Main.py\n",
    "```\n",
    "\n",
    "### 3. Experiment with Notebook\n",
    "Open `Playground.ipynb` in Jupyter for interactive exploration:\n",
    "```bash\n",
    "jupyter notebook Playground.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Dependencies\n",
    "- JAX, Flax, Optax (core ML/optimization)\n",
    "- TensorFlow, TensorFlow Datasets (data loading)\n",
    "- ml_collections (configuration)\n",
    "- Weights & Biases (experiment tracking)\n",
    "- NumPy, Pandas, Matplotlib, SciPy (data and visualization)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- [Federated Learning: Strategies for Improving Communication Efficiency](https://arxiv.org/abs/1610.05492)\n",
    "- [Practical Secure Aggregation for Privacy-Preserving Machine Learning](https://arxiv.org/abs/1611.04482)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [Flax Documentation](https://flax.readthedocs.io/)\n",
    "- [Optax Documentation](https://optax.readthedocs.io/)\n",
    "- [Weights & Biases](https://wandb.ai/)\n",
    "- [Zero-Knowledge Proofs: An Illustrated Primer](https://blog.goodaudience.com/zero-knowledge-proofs-an-illustrated-primer-7c0f5fd6878a)\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgements\n",
    "Developed by Ivan von Greiff and Ata Shaker for the \"Coding for Private Reliable and Efficient Distributed Learning\" course, Winter Semester 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c1a38",
   "metadata": {},
   "source": [
    "# ZK Cryptographic Federated Learning\n",
    "\n",
    "## Overview\n",
    "This project implements a privacy-preserving, cryptographically enhanced Federated Learning (FL) framework using JAX, Flax, and Optax. It supports multiple neural network architectures (LeNet, CNN, ResNet) and simulates a federated environment with configurable data distributions, client activity, and quantization for communication efficiency. The project is designed for research and experimentation in distributed, privacy-aware machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical and Technical Background\n",
    "\n",
    "### Federated Learning (FL)\n",
    "Federated Learning is a distributed machine learning paradigm where multiple clients (workers) collaboratively train a model under the orchestration of a central server, while keeping their data local. The server aggregates model updates from clients, improving privacy and reducing data transfer.\n",
    "\n",
    "#### Federated Optimization Objective\n",
    "The global objective in FL is to minimize the sum of local objectives:\n",
    "$$\n",
    "\\min_{\\theta} F(\\theta) = \\sum_{k=1}^K p_k F_k(\\theta)\n",
    "$$\n",
    "where:\n",
    "- $ K $: Number of clients\n",
    "- $ p_k = \\frac{n_k}{n} $: Proportion of data on client $ k $\n",
    "- $ F_k(\\theta) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\ell(\\theta; x_i, y_i) $: Local loss on client $ k $\n",
    "- $ \\ell $: Loss function (e.g., cross-entropy)\n",
    "\n",
    "#### Federated Averaging (FedAvg)\n",
    "At each round (server epoch):\n",
    "1. The server sends the current model parameters $ \\theta $ to all clients.\n",
    "2. Each client updates $ \\theta $ using its local data for several epochs, producing $ \\theta_k $.\n",
    "3. The server aggregates the updates, typically by weighted averaging:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\sum_{k=1}^K p_k \\theta_k\n",
    "   $$\n",
    "\n",
    "#### Quantization for Communication Efficiency\n",
    "To reduce communication cost, model updates are quantized before being sent to the server. This project implements **stochastic quantization**:\n",
    "- For a value $ x \\in [0, 1] $ and quantization level $ q $,\n",
    "- Let $ \\lfloor qx \\rfloor $ be the lower quantization bin, and $ \\alpha = qx - \\lfloor qx \\rfloor $ the fractional part.\n",
    "- With probability $ \\alpha $, round up; otherwise, round down:\n",
    "$$\n",
    "Q(x) = \\begin{cases}\n",
    "\\frac{\\lfloor qx \\rfloor + 1}{q} & \\text{with probability } \\alpha \\\\\n",
    "\\frac{\\lfloor qx \\rfloor}{q} & \\text{with probability } 1-\\alpha\n",
    "\\end{cases}\n",
    "$$\n",
    "This preserves the expectation: $ \\mathbb{E}[Q(x)] = x $.\n",
    "\n",
    "#### Dirichlet Data Partitioning (Non-IID Data)\n",
    "To simulate realistic, non-IID data distributions across clients, the project supports Dirichlet partitioning:\n",
    "- For $ K $ clients and $ C $ classes, draw proportions $ \\mathbf{p}_k \\sim \\text{Dirichlet}(\\alpha) $ for each class.\n",
    "- Assign data to clients according to these proportions, controlling heterogeneity with $ \\alpha $:\n",
    "  - Small $ \\alpha $: Highly non-IID (clients see few classes)\n",
    "  - Large $ \\alpha $: Nearly IID\n",
    "\n",
    "#### Client Activity and Dropout\n",
    "Client activity is modeled as a Bernoulli process:\n",
    "$$\n",
    "A_{k,t} \\sim \\text{Bernoulli}(1 - p_{\\text{inact}})\n",
    "$$\n",
    "where $ A_{k,t} = 1 $ if client $ k $ is active at round $ t $.\n",
    "\n",
    "---\n",
    "\n",
    "## Cryptographic Details: Secure Aggregation and Quantization\n",
    "\n",
    "### Secure Aggregation (Conceptual)\n",
    "While this codebase does not implement a full cryptographic protocol, it is designed to be extensible for secure aggregation, which is crucial for privacy in FL. The main idea is:\n",
    "- Each client masks its update with random values (additive secret sharing) so that the server cannot see individual updates, only their sum.\n",
    "- The server receives masked updates and, after all masks cancel out, recovers the aggregate update.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- Each client $ k $ computes update $ \\Delta_k $ and mask $ r_k $.\n",
    "- Sends $ \\Delta_k + r_k $ to the server.\n",
    "- Masks are constructed so that $ \\sum_k r_k = 0 $, thus:\n",
    "$$\n",
    "\\sum_k (\\Delta_k + r_k) = \\sum_k \\Delta_k\n",
    "$$\n",
    "\n",
    "### Quantization with Modular Arithmetic (for Secure Aggregation)\n",
    "- Quantized updates can be mapped to a finite field (e.g., modulo a prime $ p $), enabling cryptographic protocols:\n",
    "$$\n",
    "Q(x) = \\text{Quantize}(x) \\mod p\n",
    "$$\n",
    "- This is useful for protocols like [Bonawitz et al., 2017](https://arxiv.org/abs/1611.04482), where aggregation is performed in a finite field.\n",
    "\n",
    "### Zero-Knowledge Proofs (ZK) (Research Direction)\n",
    "- The project is structured to allow future integration of ZK proofs, where clients can prove properties about their updates (e.g., correct computation, bounded norm) without revealing the updates themselves.\n",
    "- ZK proofs are not yet implemented (in progress), but the quantization and modular arithmetic steps are compatible with such extensions.\n",
    "\n",
    "---\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```\n",
    "ZK Cryptographic Federated Learning/\n",
    "├── Main.py                # Main entry point, orchestrates FL workflow\n",
    "├── Training.py            # Client/server training logic, quantization\n",
    "├── Utils.py               # Data loading, client management, helper functions\n",
    "├── Commons.py             # Shared types, config, and constants\n",
    "├── Models/                # Model architectures (LeNet, CNN, ResNet)\n",
    "│   ├── lenet.py\n",
    "│   ├── cnn.py\n",
    "│   ├── resnet.py\n",
    "│   └── __init__.py\n",
    "├── Params/                # Experiment configuration files\n",
    "│   └── Parameters_1.py\n",
    "├── Logs/                  # Output logs and data distribution plots\n",
    "│   └── _Data_Distribution.png\n",
    "├── wandb/                 # Weights & Biases experiment tracking\n",
    "├── Playground.ipynb       # Interactive notebook for experimentation\n",
    "└── ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Component Details\n",
    "\n",
    "### 1. Main Workflow (`Main.py`)\n",
    "- Loads experiment configurations from `Params/`.\n",
    "- Initializes random seeds, datasets, and clients.\n",
    "- Distributes data among clients according to configuration (IID, non-IID, Dirichlet, etc.).\n",
    "- For each server epoch:\n",
    "  - Selects active clients (can simulate client dropout/inactivity).\n",
    "  - Each active client trains locally for a configurable number of epochs.\n",
    "  - Aggregates client updates (FedAvg or other schemes).\n",
    "  - Optionally applies quantization to updates.\n",
    "  - Evaluates and logs validation/test metrics.\n",
    "- Logs results to Weights & Biases (wandb) and saves CSV logs.\n",
    "\n",
    "### 2. Models (`Models/`)\n",
    "- **LeNet5 (`lenet.py`)**: Classic CNN for digit recognition. Layers: Conv → ReLU → Pool → Conv → ReLU → Pool → Dense → ReLU → Dense → ReLU → Dense.\n",
    "- **CNNs (`cnn.py`)**: Several deeper CNNs for more complex datasets (e.g., CIFAR-10). Includes variants with 4, 6, or 10 dense layers.\n",
    "- **ResNet (`resnet.py`)**: Modern deep residual networks (ResNet18, 34, 50, etc.) with skip connections, batch normalization, and flexible depth.\n",
    "- All models are implemented using Flax's `nn.Module` and are selected via the `get_model` function in `Models/__init__.py`.\n",
    "\n",
    "### 3. Training Logic (`Training.py`)\n",
    "- **train_client**: Each client receives the global model, trains locally, and returns updated parameters (optionally quantized).\n",
    "- **test_client**: Evaluates a model on test/validation data.\n",
    "- **Quantization**: Implements stochastic quantization for communication-efficient FL. Quantizes gradients/updates to a fixed number of levels, optionally modulo a prime (for secure aggregation research).\n",
    "- **JAX JIT**: Training and update steps are JIT-compiled for speed.\n",
    "\n",
    "### 4. Utilities (`Utils.py`)\n",
    "- **Client Class**: Represents a federated client, holding local data, labels, and state.\n",
    "- **get_datasets**: Loads and normalizes datasets (MNIST, CIFAR-10, Fashion-MNIST, EMNIST, etc.).\n",
    "- **Data Distribution**: Functions to split data by class, sample validation sets, and allocate data to clients (supports various non-IID schemes).\n",
    "- **Client Activity**: Simulates client dropout/inactivity per epoch.\n",
    "- **Logging and Plotting**: Logs metrics, plots data distributions, and manages experiment metadata.\n",
    "\n",
    "### 5. Configuration (`Params/Parameters_1.py`)\n",
    "- Uses `ml_collections.ConfigDict` for hierarchical, flexible configuration.\n",
    "- Controls all aspects: model, optimizer, data, server/client settings, quantization, poisoning, etc.\n",
    "- Example settings:\n",
    "  - `config.train.model`: Model architecture (e.g., LeNet5)\n",
    "  - `config.data.name`: Dataset name\n",
    "  - `config.server.num_epochs`: Number of global rounds\n",
    "  - `config.worker.num`: Number of clients\n",
    "  - `config.quantization.levels`: Quantization granularity\n",
    "\n",
    "### 6. Experiment Tracking (`wandb/`)\n",
    "- Integrates with [Weights & Biases](https://wandb.ai/) for experiment tracking, logging, and visualization.\n",
    "- Each run logs metrics, configuration, and outputs for reproducibility.\n",
    "\n",
    "### 7. Playground (`Playground.ipynb`)\n",
    "- Jupyter notebook for interactive experimentation, visualization, and debugging.\n",
    "- Walks through the main workflow, data loading, client setup, and model training.\n",
    "\n",
    "---\n",
    "\n",
    "## How Components Interact\n",
    "\n",
    "- **Main.py** orchestrates the entire workflow, calling utility functions for data and client setup, and invoking training logic for each round.\n",
    "- **Models** are selected and instantiated based on configuration, and passed to training functions.\n",
    "- **Training.py** handles all local and global training logic, including quantization and evaluation.\n",
    "- **Utils.py** provides all data handling, client management, and logging utilities.\n",
    "- **Parameters_1.py** (and other config files) allow easy switching between experimental setups.\n",
    "- **wandb** logs all relevant metrics and artifacts for later analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Running the Project\n",
    "\n",
    "### 1. Install Dependencies\n",
    "Install all required packages (see `wandb/run-*/files/requirements.txt` for full list):\n",
    "```bash\n",
    "pip install -r wandb/run-20250216_221830-67fnv1p9/files/requirements.txt\n",
    "```\n",
    "\n",
    "### 2. Run Main Experiment\n",
    "```bash\n",
    "python Main.py\n",
    "```\n",
    "\n",
    "### 3. Experiment with Notebook\n",
    "Open `Playground.ipynb` in Jupyter for interactive exploration:\n",
    "```bash\n",
    "jupyter notebook Playground.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Dependencies\n",
    "- JAX, Flax, Optax (core ML/optimization)\n",
    "- TensorFlow, TensorFlow Datasets (data loading)\n",
    "- ml_collections (configuration)\n",
    "- Weights & Biases (experiment tracking)\n",
    "- NumPy, Pandas, Matplotlib, SciPy (data and visualization)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- [Federated Learning: Strategies for Improving Communication Efficiency](https://arxiv.org/abs/1610.05492)\n",
    "- [Practical Secure Aggregation for Privacy-Preserving Machine Learning](https://arxiv.org/abs/1611.04482)\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [Flax Documentation](https://flax.readthedocs.io/)\n",
    "- [Optax Documentation](https://optax.readthedocs.io/)\n",
    "- [Weights & Biases](https://wandb.ai/)\n",
    "- [Zero-Knowledge Proofs: An Illustrated Primer](https://blog.goodaudience.com/zero-knowledge-proofs-an-illustrated-primer-7c0f5fd6878a)\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgements\n",
    "Developed by Ivan von Greiff and Ata Shaker for the \"Coding for Private Reliable and Efficient Distributed Learning\" course, Winter Semester 2024. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
